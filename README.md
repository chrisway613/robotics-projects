## π0-PnP
- 基于**键盘遥操作**的数据采集系统。
- 使用**域随机化**以增加数据的多样性。
- 采用**π0 模型**架构——结合预训练视觉语言模型(VLM: SigLip + PaliGemma)和动作专家(action expert)网络。
- 使用**流匹配(flow-matching)**方法对动作序列(action-chunk)建模。
- 结合**双向和因果注意力**的分组掩码机制，确保视觉语言与状态动作信息的有效交互。

https://github.com/user-attachments/assets/07f0a3d6-64fb-4035-ac3f-43131245e86f

## MeanFlow_RTC-PushT
- **MeanFlow 范式**：采用预测**平均速度场**的方法，实现了从多步去噪到**单步生成**的根本性改变，极大地提升了推理速度。
- **实时分块(RTC) 算法**：一种**无需重新训练的推理时算法**，参考扩散模型的**引导(guidance)机制**，在生成当前动作块时引导下一个动作块的生成，有效平滑了动作块间的衔接。

https://github.com/user-attachments/assets/c4505077-b79c-487b-9a4f-0f98fd8b6e5c

## ACT
- **硬件系统搭建**：成功配置了基于 CAN 总线通信的**主-从臂遥操作硬件系统**，并集成 OpenCV 摄像头作为视觉感知单元。
- **数据采集 Pipeline**：建立了标准化的数据采集流程，利用 **LeRobot 框架**录制了包含腕部相机图像、关节状态与动作序列的数据集。
- **ACT 算法机制**：采用 **CVAE** 架构，其编码器(**BERT** 风格)从动作序列中提取**风格潜在变量**、解码器(**DETR** 风格)以图像和机器人状态为条件，生成未来的动作块。模型通过 L1 损失与 KL 散度进行训练，并在推理时使用 **时序集成(temporal ensembling)** 技术以平滑动作块输出。
- **实验设计**：设计了多组实验，通过控制笔的样式、摆放位置和角度以及环境干扰物，科学地评估模型的泛化性能。

### PnP Task
https://github.com/user-attachments/assets/baf4d669-1389-45e5-8fb8-db1a7d26fb2f

### Teleop
https://github.com/user-attachments/assets/60c7e97d-53ac-4070-bb1f-61900d778463

## RoboCup
- **环境感知与数据集构建**：结合 **VLM** 自主采集并标注了160张图像以训练 **YOLOv11** 模型，用于精准检测足球、球门及球场关键标志物(如角球点、门柱)。
- **视觉自定位系统**：结合模型检测到的多个静态标志物的已知全局坐标，通过求解 **Perspective-n-Point 问题**，使用**最小二乘法**实时估算机器人在球场中的全局位姿。
- **分层状态机决策**：设计了包含初始化、主动探索、导航、调整、踢球等7个状态的**决策状态机**，使机器人能够根据感知信息有条不紊地完成寻找、接近、对准、踢球等一系列复杂任务。

https://github.com/user-attachments/assets/49684373-1a08-4b9d-bc3b-162f8d8d6543

## Navigation & Obstacle Avoidance
- **环境建模与系统集成**：将 PCD 点云转换为 ROS 导航栈所需的 PGM 栅格地图，并基于 Livox-SDK 实现实时点云与先验地图的配准定位，以构建稳定的感知基础。
- **分层规划与控制**：采用 **A*** 算法在全局地图上进行最优路径搜索，同时使用 **DWA** 算法根据实时点云数据生成数百条候选轨迹，通过多目标评估(目标接近度、方向一致性、碰撞风险等)选择最优局部路径。
- **实时决策与平滑控制**：以 10Hz 频率运行核心控制循环，设计了一套状态逻辑，能够**根据障碍物检测状态智能切换全局跟踪与局部避障模式**，并对最终速度指令进行平滑处理，保障了运动的稳定性。

https://github.com/user-attachments/assets/1693b024-4635-44f4-a354-b60182c41aba

## Grasp
- **PPO 强化学习控制**：先在 MuJoCo 仿真环境中搭建训练环境，设计观测空间(末端位姿)、动作空间(关节角度)与奖励函数，使用 PPO 算法训练出控制策略，然后在真实场景中部署，以替代传统运动学控制方案中的逆运动学部分。

https://github.com/user-attachments/assets/7e609792-fdbd-4a8f-a874-826292922a0c

## LeggedGym-Walk
- **策略训练与调参**：使用 **PPO** 算法和设计的奖励函数，成功训练出稳定行走的控制策略。
- **多仿真器验证(Sim2Sim)**：将在 Isaac Gym 环境中训练好的策略模型成功迁移部署到 MuJoCo 仿真器中，验证了策略的**鲁棒性**和**泛化**能力。

https://github.com/user-attachments/assets/f8e6f931-a1e4-49e8-b592-e7cc9520fe74
